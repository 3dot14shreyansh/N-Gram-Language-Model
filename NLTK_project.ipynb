{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGbDLRvd68yt"
   },
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "\n",
    "    drive.mount('/content/G_drive')\n",
    "\n",
    "    f = open('file_name', 'r')\n",
    "    raw = f.read(1000000)\n",
    "    f.close()\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOrY1-NLvB2S"
   },
   "outputs": [],
   "source": [
    "def preprocessing(raw):\n",
    "    raw = raw.replace('\\n', ' ')\n",
    "    raw = raw.replace('\\xa0', ' ')\n",
    "    raw = raw.replace(\"\\\\'\", \"'\")\n",
    "\n",
    "    # perform sentence tokenization and add START and END tokens\n",
    "    sentences = sent_tokenize(raw)\n",
    "    sentences = ['START_TAG ' + s + ' END_TAG' for s in sentences]\n",
    "\n",
    "    random.shuffle(sentences)\n",
    "    # split the data into train+dev and test\n",
    "    train_dev = sentences[: int(len(sentences) * 0.9)]\n",
    "    test = sentences[int(len(sentences) * 0.9):]\n",
    "    return train_dev, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXx9vp77vB2X"
   },
   "outputs": [],
   "source": [
    "# function to split the train+dev data into train and dev\n",
    "def split_train_dev(train_dev):\n",
    "    # shuffle the data\n",
    "    random.shuffle(train_dev)\n",
    "    train = train_dev[: int(len(train_dev) * 0.9)]\n",
    "    dev = train_dev[int(len(train_dev) * 0.9):]\n",
    "    # delete the original train+dev to save memory\n",
    "    del train_dev\n",
    "    return train, dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh52MVa2vB2d"
   },
   "outputs": [],
   "source": [
    "# function to make ngram dictionary\n",
    "# sentences is a list of sentences\n",
    "# n is the n in ngram\n",
    "def make_ngram_dict(sentences, n):\n",
    "    ngram_dict = {}\n",
    "    for s in sentences:\n",
    "        words = s.split()\n",
    "        ngram = ngrams(words, n)\n",
    "        # convert ngram into string\n",
    "        ngram = [' '.join(grams) for grams in ngram]\n",
    "\n",
    "        for g in ngram:\n",
    "            # if n == 3:\n",
    "              # print(g)\n",
    "            if g in ngram_dict:\n",
    "                ngram_dict[g] += 1\n",
    "            else:\n",
    "                ngram_dict[g] = 1\n",
    "    return ngram_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kptPJ6vg68y2"
   },
   "outputs": [],
   "source": [
    "# return the threshold value for the ngram dictionary and percentage\n",
    "# items in the dictionary that are below the threshold value should be less than percentage of the total number of items in the dictionary\n",
    "def calculate_threshold(ngram_dict, percentage):\n",
    "    total = sum(ngram_dict.values())\n",
    "    ngram_dict = sorted(ngram_dict.items(), key=lambda x: x[1])\n",
    "    threshold = 1\n",
    "    count = 0\n",
    "    for i in range(len(ngram_dict)):\n",
    "        count += ngram_dict[i][1]\n",
    "        if (100 * count) / total > percentage:\n",
    "            threshold = ngram_dict[i][1]\n",
    "            break\n",
    "    return threshold + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn284LOovB2b"
   },
   "outputs": [],
   "source": [
    "# function to convert the less frequent tokens into UNK_TAG\n",
    "# sentences is a list of sentences\n",
    "# threshold is the threshold of the frequency of a token\n",
    "def convert_unk(sentences,  threshold):\n",
    "    # find the tokens that are less frequent than the threshold\n",
    "    unigram_dict = make_ngram_dict(sentences, 1)\n",
    "    less_freq_tokens = []\n",
    "    for k, v in unigram_dict.items():\n",
    "        if v < threshold:\n",
    "            less_freq_tokens.append(k)\n",
    "\n",
    "    print('SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN: ', less_freq_tokens[:10])\n",
    "\n",
    "    # replace the less frequent tokens with UNK_TAG\n",
    "    for i in range(len(sentences)):\n",
    "        # do not use split(), instead use nltk.word_tokenize()\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        for j in range(len(words)):\n",
    "            if words[j] in less_freq_tokens:\n",
    "                words[j] = 'UNK_TAG'\n",
    "        sentences[i] = ' ' .join(words)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2R2QU2Jd68y3"
   },
   "outputs": [],
   "source": [
    "# convert the OOV tokens into UNK_TAG\n",
    "def convert_oov(sentences, unigram_dict):\n",
    "    # find the OOV tokens\n",
    "    oov_tokens = []\n",
    "    for s in sentences:\n",
    "        words = nltk.word_tokenize(s)\n",
    "        for w in words:\n",
    "            #if the frequency of the token is 0, it is OOV\n",
    "            if unigram_dict.get(w, 0) == 0:\n",
    "                oov_tokens.append(w)\n",
    "\n",
    "    # replace the OOV tokens with UNK_TAG\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        for j in range(len(words)):\n",
    "            if words[j] in oov_tokens:\n",
    "                words[j] = 'UNK_TAG'\n",
    "        sentences[i] = ' ' .join(words)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3zNAFoH68y4"
   },
   "outputs": [],
   "source": [
    "def calc_perplexity_log_likelihood(prob, M):\n",
    "\n",
    "    perlexity = 0\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for gram in prob:\n",
    "        log_likelihood += np.log2(prob[gram] if prob[gram] > 0 else 1)\n",
    "\n",
    "    perlexity = 2 ** (-log_likelihood / M)\n",
    "    return perlexity, log_likelihood\n",
    "\n",
    "\n",
    "def calc_perplexity_log_likelihood_degugger(prob, M):\n",
    "\n",
    "    perlexity = 0\n",
    "    log_likelihood = 0\n",
    "    counter = 0\n",
    "    for gram in prob:\n",
    "        counter += 1\n",
    "        if counter < 30:\n",
    "          print('DEBUG: ', np.log2(prob[gram] if prob[gram] > 0 else 1))\n",
    "        log_likelihood += np.log2(prob[gram] if prob[gram] > 0 else 1)\n",
    "\n",
    "    perlexity = 2 ** (-log_likelihood / M)\n",
    "    return perlexity, log_likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2mCAYI2vB2f"
   },
   "outputs": [],
   "source": [
    "# linear interpolation\n",
    "\n",
    "\n",
    "# function to calculate the probability with given lambdas and the model using linear interpolation. Return the dictionary of trigram probability\n",
    "# lambda1, lambda2, lambda3 are the lambdas\n",
    "# train is a list of sentences\n",
    "def calc_prob_interpolation(lambda1, lambda2, lambda3, dev, unigram_dict, bigram_dict, trigram_dict):\n",
    "\n",
    "    prob = {}\n",
    "    # create a trigram dev list using different sentences of dev\n",
    "    trigram_dev = []\n",
    "    trigram_dict_dev = make_ngram_dict(dev, 3)\n",
    "\n",
    "    for gram in trigram_dict_dev:\n",
    "        trigram_dev.append(gram)\n",
    "    for trigram_string in trigram_dev:\n",
    "        words_in_trigram = trigram_string.split()\n",
    "        \n",
    "        bigram_string = ' ' .join(words_in_trigram[1:])\n",
    "        \n",
    "        unigram_string = words_in_trigram[2]\n",
    "        p1 = trigram_dict.get(trigram_string, 0) / \\\n",
    "            bigram_dict.get(bigram_string, 1)\n",
    "        p2 = bigram_dict.get(bigram_string, 0) / \\\n",
    "            unigram_dict.get(unigram_string, 1)\n",
    "        p3 = unigram_dict.get(unigram_string, 0) / len(unigram_dict)\n",
    "        prob[trigram_string] = lambda1 * p1 + lambda2 * p2 + lambda3 * p3\n",
    "\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def interpolation_smoothing(unigram_dict_train, bigram_dict_train, trigram_dict_train, dev):\n",
    "    best_lambda1 = 0\n",
    "    best_lambda2 = 0\n",
    "    best_lambda3 = 0\n",
    "    best_log_likelihood = -np.inf\n",
    "    best_perplexity = np.inf\n",
    "    best_prob = {}\n",
    "\n",
    "    # calculate number of trigrams in the dev set\n",
    "    M = 0\n",
    "    for s in dev:\n",
    "        M += len(word_tokenize(s))\n",
    "\n",
    "    for lambda1 in np.arange(0, 1, 0.025):\n",
    "        for lambda2 in np.arange(0, 1 - lambda1, 0.025):\n",
    "\n",
    "            lambda3 = 1 - lambda1 - lambda2\n",
    "            if lambda3 == 0:\n",
    "              continue\n",
    "\n",
    "            prob = calc_prob_interpolation(\n",
    "                lambda1, lambda2, lambda3, dev, unigram_dict_train, bigram_dict_train, trigram_dict_train)\n",
    "            perplexity, log_likelihood = calc_perplexity_log_likelihood(\n",
    "                prob, M)\n",
    "            #print(perplexity, log_likelihood)\n",
    "            if log_likelihood > best_log_likelihood:\n",
    "                best_lambda1 = lambda1\n",
    "                best_lambda2 = lambda2\n",
    "                best_lambda3 = lambda3\n",
    "                best_log_likelihood = log_likelihood\n",
    "                best_perplexity = perplexity\n",
    "                best_prob = prob\n",
    "\n",
    "    return best_lambda1, best_lambda2, best_lambda3, best_perplexity, best_log_likelihood \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLsFiluIywBX"
   },
   "outputs": [],
   "source": [
    "def calc_prob_discounting_bigram(bigram_dict_train, unigram_dict_train, beta1):\n",
    "  prob = {}\n",
    "\n",
    "  for unigram in unigram_dict_train:\n",
    "    missing_mass = 0\n",
    "    deno = 0\n",
    "    bigram_count_zero = []\n",
    "    for unigram2 in unigram_dict_train:\n",
    "      bigram = unigram+ \" \" + unigram2\n",
    "      if bigram in bigram_dict_train.keys():\n",
    "        prob[bigram] = bigram_dict_train.get(bigram, 0) / unigram_dict_train.get(unigram , 1)\n",
    "        missing_mass += beta1\n",
    "      else:\n",
    "        deno += unigram_dict_train.get(unigram2, 0)\n",
    "        bigram_count_zero.append(unigram2)\n",
    "\n",
    "    missing_mass /= unigram_dict_train.get(unigram, 1)\n",
    "    for unigram2 in bigram_count_zero:\n",
    "      bigram = unigram+ \" \" + unigram2\n",
    "      prob[bigram] = missing_mass * unigram_dict_train.get(unigram2, 0) / deno\n",
    "\n",
    "  return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xXrRE8DnXSk"
   },
   "outputs": [],
   "source": [
    "#Discounting Smoothing\n",
    "def calc_prob_discounting(trigram_dict_train, bigram_dict_train, unigram_dict_train, dev, beta1, beta2):\n",
    "    prob = {}\n",
    "    bigram_prob  = calc_prob_discounting_bigram(bigram_dict_train, unigram_dict_train, beta1)\n",
    "    missing_mass = 0\n",
    "    trigram_dev = []\n",
    "    trigram_dict_dev = make_ngram_dict(dev, 3)\n",
    "    for gram in trigram_dict_dev:\n",
    "        trigram_dev.append(gram)\n",
    "\n",
    "    temp_prob = {}\n",
    "    for bigram in bigram_dict_train:\n",
    "      if bigram.split()[1] == 'END_TAG':\n",
    "        continue\n",
    "      missing_mass = 0\n",
    "      deno = 0\n",
    "      trigram_count_zero = []\n",
    "      for unigram in unigram_dict_train:\n",
    "        trigram = bigram+ \" \" + unigram\n",
    "        if trigram in trigram_dict_train.keys():\n",
    "          temp_prob[trigram] = trigram_dict_train.get(trigram, 0) / bigram_dict_train.get(bigram , 1)\n",
    "          missing_mass += beta2\n",
    "        else:\n",
    "          bigram2=  \" \".join(trigram.split()[1:])\n",
    "          deno += bigram_prob.get(bigram2, 0)\n",
    "          #print('DEBUG: DENO = ',deno, 'BIGRAM2 = ', bigram2 in bigram_prob.keys(),'BIGRAM2 =', bigram2)\n",
    "          trigram_count_zero.append(unigram)\n",
    "\n",
    "      missing_mass /= bigram_dict_train.get(bigram, 1)\n",
    "      for unigram in trigram_count_zero:\n",
    "        trigram = bigram + \" \" + unigram\n",
    "        bigram2 = \" \".join(trigram.split()[1:])\n",
    "\n",
    "        temp_prob[trigram] = missing_mass * bigram_prob.get(bigram2, 0) / deno\n",
    "\n",
    "    for trigram_string in trigram_dev:\n",
    "      prob[trigram_string] = temp_prob.get(trigram_string, unigram_dict_train.get(trigram_string.split()[2] ,1) / sum(unigram_dict_train.values()))\n",
    "\n",
    "    return prob   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTaJjRV8Oc62"
   },
   "outputs": [],
   "source": [
    "# laplace smoothing\n",
    "# returns the dictionary of trigram probability\n",
    "\n",
    "def calc_prob_laplace(trigram_dict_train, bigram_dict_train, unigram_dict_train, dev):\n",
    "    # create a trigram dev list using different sentences of dev\n",
    "    trigram_dev = []\n",
    "    trigram_dict_dev = make_ngram_dict(dev, 3)\n",
    "    for gram in trigram_dict_dev:\n",
    "        trigram_dev.append(gram)\n",
    "\n",
    "    prob = {}\n",
    "    # for each trigram in the dev set, calculate the probability using add-one smoothing\n",
    "    for trigram_string in trigram_dev:\n",
    "        words_in_trigram = trigram_string.split()\n",
    "        bigram_string = ' '.join(words_in_trigram[:1])\n",
    "        count1 = trigram_dict_train.get(trigram_string, 0)\n",
    "        count2 = bigram_dict_train.get(bigram_string, 0)\n",
    "        prob[trigram_string] = (count1 + 1) / \\\n",
    "            (count2 + len(unigram_dict_train))\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "def laplace_smoothing(unigram_dict_train, bigram_dict_train, trigram_dict_train, dev):\n",
    "    prob = calc_prob_laplace(\n",
    "        trigram_dict_train, bigram_dict_train, unigram_dict_train, dev)\n",
    "    M = 0\n",
    "    for s in dev:\n",
    "        M += len(word_tokenize(s)) \n",
    "    perplexity, log_likelihood = calc_perplexity_log_likelihood(prob, M)\n",
    "    return perplexity, log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0Qb5O4t7tqM"
   },
   "outputs": [],
   "source": [
    "# byte pair encoding\n",
    "def byte_pair_encoding(train):\n",
    "\n",
    "\n",
    "    vocab = {}\n",
    "    temp = \"\"\n",
    "    \n",
    "    words = []\n",
    "    # create word list from train\n",
    "    for s in train:\n",
    "        words.extend(word_tokenize(s))\n",
    "\n",
    "\n",
    "    word_freq_dict = collections.defaultdict(int)\n",
    "\n",
    "    for word in words:\n",
    "        word = (' ').join(list(word))\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    #print(word_freq_dict)\n",
    "\n",
    "    # word_freq_dict\n",
    "\n",
    "    char_freq_dict = collections.defaultdict(int)\n",
    "    for word, freq in word_freq_dict.items():\n",
    "        chars = word.split()\n",
    "        for char in chars:\n",
    "            char_freq_dict[char] += freq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # find the best pair\n",
    "\n",
    "    def get_pairs(word_freq_dict):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        #print('1', word_freq_dict)\n",
    "        for word, freq in word_freq_dict.items():\n",
    "            chars = word.split()\n",
    "            for i in range(len(chars)-1):\n",
    "\n",
    "                pairs[chars[i], chars[i+1]] += freq\n",
    "                #print(chars[i], chars[i+1])\n",
    "        return pairs\n",
    "\n",
    "\n",
    "    def merge_byte_pairs(best_pair, word_freq_dict):\n",
    "        # print(best_pair)\n",
    "        merged_dict = {}\n",
    "        bigram = re.escape(' '.join(best_pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in word_freq_dict:\n",
    "            # print(word)\n",
    "            w_out = p.sub(''.join(best_pair), word)\n",
    "            merged_dict[w_out] = word_freq_dict[word]\n",
    "        #print(merged_dict)\n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def get_subword_tokens(word_freq_dict):\n",
    "        char_freq_dict = collections.defaultdict(int)\n",
    "        for word, freq in word_freq_dict.items():\n",
    "            chars = word.split()\n",
    "            for char in chars:\n",
    "                char_freq_dict[char] += freq\n",
    "        return char_freq_dict\n",
    "\n",
    "\n",
    "    for i in range(300):\n",
    "        pairs = get_pairs(word_freq_dict)\n",
    "        #print(pairs)\n",
    "        best_pair = max(pairs, key= lambda x: pairs[x])\n",
    "        #print(best_pair)\n",
    "        word_freq_dict = merge_byte_pairs(best_pair, word_freq_dict)\n",
    "        subword_tokens = get_subword_tokens(word_freq_dict)\n",
    "\n",
    "    return subword_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itJAY7SN7pfa"
   },
   "outputs": [],
   "source": [
    "def tokenise(word, subword_tokens):\n",
    "    if word in subword_tokens:\n",
    "        return [word]\n",
    "    start = 0\n",
    "    tokens = []\n",
    "    while start < len(word):\n",
    "        for end in range(len(word), start, -1):\n",
    "            subword = word[start:end]\n",
    "            if subword in subword_tokens:\n",
    "                tokens.append(subword)\n",
    "                start = end\n",
    "                break\n",
    "        else:\n",
    "            start += 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8YMv7RX7n_2"
   },
   "outputs": [],
   "source": [
    "#returns the modified list of sentences train, that contains subwords instead of words\n",
    "def tokenize_train(train, subword_tokens):\n",
    "    tokenized_train_sentences = []\n",
    "    for sentence in train:\n",
    "        tokenized_sentence = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            tokenized_sentence.extend(tokenise(word, subword_tokens))\n",
    "        tokenized_train_sentences.append(' '.join(tokenized_sentence))\n",
    "    return tokenized_train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMb1nGD368y6"
   },
   "outputs": [],
   "source": [
    "def execute():\n",
    "\n",
    "    raw = extract_data()\n",
    "    train_dev, test = preprocessing(raw)\n",
    "    # print the number of sentences in the training and development set\n",
    "    print('Number of sentences in the training and development set: ', len(train_dev))\n",
    "    # print the number of sentences in the test set\n",
    "    print('Number of sentences in the test set: ', len(test))\n",
    "\n",
    "    print('')\n",
    "    print('Part 1: Word-level language modeling')\n",
    "    print('')\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        print('*'*50)\n",
    "        print('Round ', i + 1)\n",
    "\n",
    "        # initialize the data for this iteration\n",
    "        train, dev = split_train_dev(train_dev)\n",
    "        # print the number of sentences in the training set\n",
    "        print('Number of sentences in the training set: ', len(train))\n",
    "        # print the number of sentences in the development set\n",
    "        print('Number of sentences in the development set: ', len(dev))\n",
    "\n",
    "        unigram_dict_train = make_ngram_dict(train, 1)\n",
    "\n",
    "        bigram_dict_train = make_ngram_dict(train, 2)\n",
    "        trigram_dict_train = make_ngram_dict(train, 3)\n",
    "        # for item in trigram_dict_train:\n",
    "        # print(item[0])\n",
    "\n",
    "        threshold = 2\n",
    "        threshold = calculate_threshold(unigram_dict_train, 10)\n",
    "        print('Threshold value for frequency: ', threshold)\n",
    "        train = convert_unk(train, threshold)\n",
    "        dev = convert_oov(dev, unigram_dict_train)\n",
    "        test_new = convert_oov(test, unigram_dict_train)\n",
    "        m_test = 0\n",
    "        for s in test_new:\n",
    "            m_test += len(word_tokenize(s))\n",
    "\n",
    "        m_dev = 0\n",
    "        for s in dev:\n",
    "            m_dev += len(word_tokenize(s))\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        print('Linear interpolation')\n",
    "        print(' ')\n",
    "        lambda1, lambda2, lambda3, perplexity_interpolation, log_likelihood_interpolation = interpolation_smoothing(\n",
    "            unigram_dict_train, bigram_dict_train, trigram_dict_train, dev)\n",
    "\n",
    "        print('lambda1: ', lambda1)\n",
    "        print('lambda2: ', lambda2)\n",
    "        print('lambda3: ', lambda3)\n",
    "        print('Perplexity: ', perplexity_interpolation)\n",
    "        print('Log likelihood: ', log_likelihood_interpolation)\n",
    "        print(' ')\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        # print('Backoff smoothing')\n",
    "        # print(' ')\n",
    "        # dev_prob_disco = calc_prob_discounting(trigram_dict_train, bigram_dict_train, unigram_dict_train, dev, 0.25, 0.25)\n",
    "        # perplexity_disco_dev, log_likelihood_disco_dev = calc_perplexity_log_likelihood(dev_prob_disco, m_dev)\n",
    "        # del dev_prob_disco\n",
    "        # print('Perplexity: ', perplexity_disco_dev)\n",
    "        # print('Log likelihood: ', log_likelihood_disco_dev)\n",
    "        # print(' ')\n",
    "\n",
    "        # print('-'*50)\n",
    "\n",
    "        print('Laplace smoothing')\n",
    "        print(' ')\n",
    "        perplexity_laplace, log_likelihood_laplace = laplace_smoothing(\n",
    "            unigram_dict_train, bigram_dict_train, trigram_dict_train, dev)\n",
    "        print('Perplexity: ', perplexity_laplace)\n",
    "        print('Log likelihood: ', log_likelihood_laplace)\n",
    "        print(' ')\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        print('Test Set Performance Comparsion')\n",
    "        test_prob_interpolation = calc_prob_interpolation(lambda1, lambda2, lambda3, test_new, unigram_dict_train, bigram_dict_train, trigram_dict_train)\n",
    "        #m is the number of words in the new test set\n",
    "\n",
    "        \n",
    "        perplexity_interpolation_test, log_likelihood_interpolation_test = calc_perplexity_log_likelihood(test_prob_interpolation, m_test)\n",
    "        del test_prob_interpolation\n",
    "\n",
    "        test_prob_laplace = calc_prob_laplace(trigram_dict_train, bigram_dict_train, unigram_dict_train, test_new)\n",
    "        perplexity_laplace_test, log_likelihood_laplace_test = calc_perplexity_log_likelihood(test_prob_laplace, m_test)\n",
    "        del test_prob_laplace\n",
    "\n",
    "        # test_prob_disco = calc_prob_discounting(trigram_dict_train, bigram_dict_train, unigram_dict_train, test_new, 0.25, 0.25)\n",
    "        # perplexity_disco_test, log_likelihood_disco_test = calc_perplexity_log_likelihood(test_prob_disco, m_test)\n",
    "        # del test_prob_disco\n",
    "        \n",
    "        print(' ')\n",
    "        print('Interpolation: Perplexity = ' ,perplexity_interpolation_test, '  Log Likelihood = ', log_likelihood_interpolation_test)\n",
    "        print('Laplace: Perplexity = ' ,perplexity_laplace_test, '  Log Likelihood = ', log_likelihood_laplace_test)\n",
    "        # print('Backoff: Perplexity = ' ,perplexity_disco_test, '  Log Likelihood = ', log_likelihood_disco_test)\n",
    "\n",
    "        del unigram_dict_train\n",
    "        del bigram_dict_train\n",
    "        del trigram_dict_train\n",
    "        del train\n",
    "        del dev\n",
    "        del threshold\n",
    "\n",
    "        print(' '*50)\n",
    "        print(' '*50)\n",
    "\n",
    "    print(':'*50)\n",
    "    print('')\n",
    "\n",
    "    print('')\n",
    "    print('Part 2: Subword-level language modeling')\n",
    "    print('')\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        print('*'*50)\n",
    "        print('Round ', i + 1)\n",
    "\n",
    "        # initialize the data for this iteration\n",
    "        train, dev = split_train_dev(train_dev)\n",
    "        # print the number of sentences in the training set\n",
    "        print('Number of sentences in the training set: ', len(train))\n",
    "        # print the number of sentences in the development set\n",
    "        print('Number of sentences in the development set: ', len(dev))\n",
    "\n",
    "\n",
    "\n",
    "        subwords = byte_pair_encoding(train)\n",
    "        #print(subwords)\n",
    "        train = tokenize_train(train, subwords)\n",
    "        dev = tokenize_train(dev, subwords)\n",
    "        tokenized_test = tokenize_train(test, subwords)\n",
    "\n",
    "        m_test = 0\n",
    "        for s in tokenized_test:\n",
    "            m_test += len(word_tokenize(s))\n",
    "\n",
    "        m_dev = 0\n",
    "        for s in dev:\n",
    "            m_dev += len(word_tokenize(s))\n",
    "\n",
    "        unigram_dict_train = make_ngram_dict(train, 1)\n",
    "\n",
    "        bigram_dict_train = make_ngram_dict(train, 2)\n",
    "        trigram_dict_train = make_ngram_dict(train, 3)\n",
    "\n",
    "        #print('Sample train set: ', train[:20])\n",
    "        \n",
    "        print('-'*50)\n",
    "\n",
    "        print('Linear interpolation')\n",
    "        print(' ')\n",
    "        lambda1, lambda2, lambda3, perplexity_interpolation, log_likelihood_interpolation = interpolation_smoothing(\n",
    "            unigram_dict_train, bigram_dict_train, trigram_dict_train, dev)\n",
    "\n",
    "        print('lambda1: ', lambda1)\n",
    "        print('lambda2: ', lambda2)\n",
    "        print('lambda3: ', lambda3)\n",
    "        print('Perplexity: ', perplexity_interpolation)\n",
    "        print('Log likelihood: ', log_likelihood_interpolation)\n",
    "        print(' ')\n",
    "\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        # print('Backoff smoothing')\n",
    "        # print(' ')\n",
    "        # dev_prob_disco = calc_prob_discounting(trigram_dict_train, bigram_dict_train, unigram_dict_train, dev, 0.25, 0.25)\n",
    "        # perplexity_disco_dev, log_likelihood_disco_dev = calc_perplexity_log_likelihood(dev_prob_disco, m_dev)\n",
    "        # del dev_prob_disco\n",
    "        # print('Perplexity: ', perplexity_disco_dev)\n",
    "        # print('Log likelihood: ', log_likelihood_disco_dev)\n",
    "        # print(' ')\n",
    "\n",
    "        # print('-'*50)\n",
    "\n",
    "\n",
    "        print('Laplace smoothing')\n",
    "        print(' ')\n",
    "        perplexity_laplace, log_likelihood_laplace = laplace_smoothing(\n",
    "            unigram_dict_train, bigram_dict_train, trigram_dict_train, dev)\n",
    "        print('Perplexity: ', perplexity_laplace)\n",
    "        print('Log likelihood: ', log_likelihood_laplace)\n",
    "        print(' ')\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        print('Test Set Performance Comparsion')\n",
    "        test_prob_interpolation = calc_prob_interpolation(lambda1, lambda2, lambda3, tokenized_test, unigram_dict_train, bigram_dict_train, trigram_dict_train)\n",
    "        \n",
    "        perplexity_interpolation_test, log_likelihood_interpolation_test = calc_perplexity_log_likelihood(test_prob_interpolation, m_test)\n",
    "        del test_prob_interpolation\n",
    "\n",
    "        test_prob_laplace = calc_prob_laplace(trigram_dict_train, bigram_dict_train, unigram_dict_train, tokenized_test)\n",
    "        perplexity_laplace_test, log_likelihood_laplace_test = calc_perplexity_log_likelihood(test_prob_laplace, m_test)\n",
    "        del test_prob_laplace\n",
    "\n",
    "        # test_prob_disco = calc_prob_discounting(trigram_dict_train, bigram_dict_train, unigram_dict_train, tokenized_test, 0.25, 0.25)\n",
    "        # perplexity_disco_test, log_likelihood_disco_test = calc_perplexity_log_likelihood(test_prob_disco, m_test)\n",
    "        # del test_prob_disco\n",
    "\n",
    "        print(' ')\n",
    "        print('Interpolation: Perplexity = ' ,perplexity_interpolation_test, '  Log Likelihood = ', log_likelihood_interpolation_test)\n",
    "        print('Laplace: Perplexity = ' ,perplexity_laplace_test, '  Log Likelihood = ', log_likelihood_laplace_test)\n",
    "        # print('Backoff: Perplexity = ' ,perplexity_disco_test, '  Log Likelihood = ', log_likelihood_disco_test)\n",
    "        \n",
    "        \n",
    "        del unigram_dict_train\n",
    "        del bigram_dict_train\n",
    "        del trigram_dict_train\n",
    "        del train\n",
    "        del dev\n",
    "\n",
    "        print(' '*50)\n",
    "        print(' '*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtxDsqG668y7",
    "outputId": "f17648c7-9be3-4667-d8bd-71f26e1b8d97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Number of sentences in the training and development set:  6351\n",
      "Number of sentences in the test set:  706\n",
      "\n",
      "Part 1: Word-level language modeling\n",
      "\n",
      "**************************************************\n",
      "Round  1\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "Threshold value for frequency:  2\n",
      "SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN:  ['Ebeling,', 'late-18th-century', 'remarks', '(22', '1819', '1880;', 'alternatively', '\"Mary', 'Anne\"', '\"Marian\"),']\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.025\n",
      "lambda2:  0.17500000000000002\n",
      "lambda3:  0.7999999999999999\n",
      "Perplexity:  51.280745543724024\n",
      "Log likelihood:  -104132.09061850791\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  4213.339111784952\n",
      "Log likelihood:  -220730.9982196913\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  51.56924626838287   Log Likelihood =  -113916.68050087872\n",
      "Laplace: Perplexity =  4014.617219863241   Log Likelihood =  -239732.18167244722\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  2\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "Threshold value for frequency:  2\n",
      "SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN:  ['Trebunskaya.', \"wouldn't\", \"weren't\", 'Year\\'s\").', 'Gojko', 'Mitić', 'kindhearted', 'Söhne', 'großen', 'Bärin\"']\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.025\n",
      "lambda2:  0.17500000000000002\n",
      "lambda3:  0.7999999999999999\n",
      "Perplexity:  49.97761252284196\n",
      "Log likelihood:  -102221.10737228418\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  3734.052994240971\n",
      "Log likelihood:  -214950.26441404596\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  49.752323471814364   Log Likelihood =  -112880.39393364791\n",
      "Laplace: Perplexity =  3867.778790115428   Log Likelihood =  -238655.64092429957\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  3\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "Threshold value for frequency:  2\n",
      "SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN:  ['Miguel', 'Icaza,', 'variables.', 'Brisbane,', '2001–2003.', 'Bowl', 'Klemperer', 'orchestration', 'Preludes\".', 'oneself']\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.025\n",
      "lambda2:  0.17500000000000002\n",
      "lambda3:  0.7999999999999999\n",
      "Perplexity:  50.20684850642735\n",
      "Log likelihood:  -101549.72564192503\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  3737.124654430554\n",
      "Log likelihood:  -213310.27294605086\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  47.70812801417546   Log Likelihood =  -111668.24371090488\n",
      "Laplace: Perplexity =  3724.310435230137   Log Likelihood =  -237563.5840302011\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  4\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "Threshold value for frequency:  2\n",
      "SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN:  ['gyro', 'biased', 'argues', 'teachings', 'All-Australian', 'Camarines', 'Sur,', 'favors', 'Arroyo', 'families;']\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.025\n",
      "lambda2:  0.17500000000000002\n",
      "lambda3:  0.7999999999999999\n",
      "Perplexity:  52.39465359356804\n",
      "Log likelihood:  -105819.85018591776\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  4179.3331172515855\n",
      "Log likelihood:  -222874.36882088397\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  45.707222380850475   Log Likelihood =  -110430.37799018518\n",
      "Laplace: Perplexity =  3589.1177562621474   Log Likelihood =  -236495.31559880107\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  5\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "Threshold value for frequency:  2\n",
      "SOME LESS FREQUENCY TOKENS, TO BE LABELLED UNKNOWN:  ['subscribed', \"Krafft-Ebing's\", \"Freud's\", 'attraction', 'trauma', 'curable.', 'cinema', 'DEFA,', 'AG\",', 'Berlin\",']\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.025\n",
      "lambda2:  0.17500000000000002\n",
      "lambda3:  0.7999999999999999\n",
      "Perplexity:  49.73661610856814\n",
      "Log likelihood:  -103424.938862302\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  3779.65388448484\n",
      "Log likelihood:  -218072.10487413342\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  43.85271332874737   Log Likelihood =  -109233.70355273872\n",
      "Laplace: Perplexity =  3469.373509527992   Log Likelihood =  -235514.9610028776\n",
      "                                                  \n",
      "                                                  \n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "Part 2: Subword-level language modeling\n",
      "\n",
      "**************************************************\n",
      "Round  1\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.0\n",
      "lambda2:  0.0\n",
      "lambda3:  1.0\n",
      "Perplexity:  0.4763102322461205\n",
      "Log likelihood:  43981.30142539486\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  41.72347567519566\n",
      "Log likelihood:  -221248.71206576127\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  0.6311990540055504   Log Likelihood =  33847.51964206688\n",
      "Laplace: Perplexity =  11.59137167800089   Log Likelihood =  -180241.52935522862\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  2\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.0\n",
      "lambda2:  0.0\n",
      "lambda3:  1.0\n",
      "Perplexity:  0.47367309974611704\n",
      "Log likelihood:  44039.94106763554\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  41.359798637577654\n",
      "Log likelihood:  -219381.6642514549\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  0.6302382541531327   Log Likelihood =  33948.25434104372\n",
      "Laplace: Perplexity =  11.587514864203337   Log Likelihood =  -180156.9630249532\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  3\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.0\n",
      "lambda2:  0.0\n",
      "lambda3:  1.0\n",
      "Perplexity:  0.47477205268198913\n",
      "Log likelihood:  43300.45893582698\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  40.37193756319417\n",
      "Log likelihood:  -214963.8036433282\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  0.6293068954610845   Log Likelihood =  34017.583079071715\n",
      "Laplace: Perplexity =  11.663396157068565   Log Likelihood =  -180427.8529040718\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  4\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.0\n",
      "lambda2:  0.0\n",
      "lambda3:  1.0\n",
      "Perplexity:  0.46921890539522393\n",
      "Log likelihood:  43406.8613624375\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  42.28859040761865\n",
      "Log likelihood:  -214802.13990310355\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  0.6298310307752651   Log Likelihood =  33948.42974913498\n",
      "Laplace: Perplexity =  11.646984202211264   Log Likelihood =  -180281.92286328343\n",
      "                                                  \n",
      "                                                  \n",
      "**************************************************\n",
      "Round  5\n",
      "Number of sentences in the training set:  5715\n",
      "Number of sentences in the development set:  636\n",
      "--------------------------------------------------\n",
      "Linear interpolation\n",
      " \n",
      "lambda1:  0.0\n",
      "lambda2:  0.0\n",
      "lambda3:  1.0\n",
      "Perplexity:  0.466329323073339\n",
      "Log likelihood:  44215.759107733924\n",
      " \n",
      "--------------------------------------------------\n",
      "Laplace smoothing\n",
      " \n",
      "Perplexity:  42.37228499654513\n",
      "Log likelihood:  -217147.84459693497\n",
      " \n",
      "--------------------------------------------------\n",
      "Test Set Performance Comparsion\n",
      " \n",
      "Interpolation: Perplexity =  0.6273365370614508   Log Likelihood =  34278.86083653261\n",
      "Laplace: Perplexity =  11.5494700075152   Log Likelihood =  -179869.24226298853\n",
      "                                                  \n",
      "                                                  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from google.colab import drive\n",
    "nltk.download('punkt')\n",
    "execute()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
